{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "from tensorflow import keras\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.inception_v3 import decode_predictions\n",
    "from keras.models import Model, load_model\n",
    "import keras.backend as K\n",
    "\n",
    "import model as tcav_model\n",
    "import tcav as tcav\n",
    "import utils as utils\n",
    "import activation_generator as act_gen\n",
    "import tensorflow as tf\n",
    "import utils_plot as utils_plot\n",
    "import cav as cav\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "import pickle\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import operator\n",
    "from PIL import Image\n",
    "from keras.preprocessing import image\n",
    "\n",
    "import os\n",
    "import math\n",
    "\n",
    "import PIL.Image\n",
    "from sklearn.metrics import pairwise\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "import tensorflow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tyler/anaconda3/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "K.set_learning_phase(0)\n",
    "model = load_model('v3_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = K.get_session()\n",
    "\n",
    "endpoints_v3 = dict(\n",
    "    input=model.inputs[0].name,\n",
    "    input_tensor=model.inputs[0],\n",
    "    logit=model.outputs[0].name,\n",
    "    prediction=model.outputs[0].name,\n",
    "    prediction_tensor=model.outputs[0],)\n",
    "\n",
    "tf.logging.set_verbosity(0)\n",
    "\n",
    "working_dir = '/Users/tyler/Desktop/dissertation/programming/tcav_on_azure'\n",
    "\n",
    "label_path = os.path.join(working_dir,'labels.txt')\n",
    "\n",
    "mymodel = tcav_model.KerasModelWrapper(sess, \n",
    "        label_path, [299, 299, 3], endpoints_v3, \n",
    "        'InceptionV3_public', (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['layer','concept_p','concept_n','acc_orig','acc_marg','acc_L1']\n",
    "df = pd.DataFrame(columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(df,open('df_acc1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running mixed0, sky_sub_2, random500_0\n",
      "running mixed0, sky_sub_2, random500_1\n",
      "running mixed0, sky_sub_2, random500_2\n",
      "running mixed0, sky_sub_2, random500_3\n",
      "running mixed0, sky_sub_2, random500_4\n",
      "running mixed1, sky_sub_2, random500_0\n",
      "running mixed1, sky_sub_2, random500_1\n",
      "running mixed1, sky_sub_2, random500_2\n",
      "running mixed1, sky_sub_2, random500_3\n",
      "running mixed1, sky_sub_2, random500_4\n",
      "running mixed2, sky_sub_2, random500_0\n",
      "running mixed2, sky_sub_2, random500_1\n",
      "running mixed2, sky_sub_2, random500_2\n",
      "running mixed2, sky_sub_2, random500_3\n",
      "running mixed2, sky_sub_2, random500_4\n",
      "running mixed3, sky_sub_2, random500_0\n",
      "running mixed3, sky_sub_2, random500_1\n",
      "running mixed3, sky_sub_2, random500_2\n",
      "running mixed3, sky_sub_2, random500_3\n",
      "running mixed3, sky_sub_2, random500_4\n",
      "running mixed4, sky_sub_2, random500_0\n",
      "running mixed4, sky_sub_2, random500_1\n",
      "running mixed4, sky_sub_2, random500_2\n",
      "running mixed4, sky_sub_2, random500_3\n",
      "running mixed4, sky_sub_2, random500_4\n",
      "running mixed5, sky_sub_2, random500_0\n",
      "running mixed5, sky_sub_2, random500_1\n",
      "running mixed5, sky_sub_2, random500_2\n",
      "running mixed5, sky_sub_2, random500_3\n",
      "running mixed5, sky_sub_2, random500_4\n",
      "running mixed6, sky_sub_2, random500_0\n",
      "running mixed6, sky_sub_2, random500_1\n",
      "running mixed6, sky_sub_2, random500_2\n",
      "running mixed6, sky_sub_2, random500_3\n",
      "running mixed6, sky_sub_2, random500_4\n",
      "running mixed7, sky_sub_2, random500_0\n",
      "running mixed7, sky_sub_2, random500_1\n",
      "running mixed7, sky_sub_2, random500_2\n",
      "running mixed7, sky_sub_2, random500_3\n",
      "running mixed7, sky_sub_2, random500_4\n",
      "running mixed8, sky_sub_2, random500_0\n",
      "running mixed8, sky_sub_2, random500_1\n",
      "running mixed8, sky_sub_2, random500_2\n",
      "running mixed8, sky_sub_2, random500_3\n",
      "running mixed8, sky_sub_2, random500_4\n",
      "running mixed9, sky_sub_2, random500_0\n",
      "running mixed9, sky_sub_2, random500_1\n",
      "running mixed9, sky_sub_2, random500_2\n",
      "running mixed9, sky_sub_2, random500_3\n",
      "running mixed9, sky_sub_2, random500_4\n",
      "### did write ###\n"
     ]
    }
   ],
   "source": [
    "## CAV loop\n",
    "source_dir = working_dir + '/concepts/'\n",
    "\n",
    "target_class = 'zebra'\n",
    "target_class_name = 'zebra'\n",
    "#class_1 = 'grassland_sub_1'\n",
    "\n",
    "#class_1_list = ['green_sub_1','green_sub_2','yellow_sub_1','yellow_sub_2']#,'blue_sub_1','blue_sub_2','orange_sub_1','orange_sub_1']\n",
    "class_1_list = ['dotted_sub_1','sky_sub_1','ocean_sub_1']#,'blue_sub_1','blue_sub_2','orange_sub_1','orange_sub_1']\n",
    "\n",
    "#class_1_list = ['random500_6','random500_7','random500_8','random500_9']\n",
    "#class_1_list = ['sky_sub_1','ocean_sub_1','grassland_sub_1','road_sub_1']\n",
    "#class_1_list = ['striped_sub_1','grid_sub_1','dotted_sub_1','banded_sub_1']\n",
    "class_1_list = ['sky_sub_2']\n",
    "\n",
    "hparams = tf.contrib.training.HParams(model_type='linear', alpha=.1)\n",
    "\n",
    "bn_names = ['mixed0','mixed1','mixed2','mixed3','mixed4','mixed5','mixed6','mixed7','mixed8','mixed9']#,'mixed10']\n",
    "#bn_names = ['mixed9']\n",
    "\n",
    "df = pickle.load(open('df_acc1.pkl', 'rb'))\n",
    "\n",
    "num_random = 5\n",
    "\n",
    "for concept_p in class_1_list:\n",
    "    for layer in  bn_names:\n",
    "        for class_idx in range(num_random):\n",
    "            concept_n = 'random500_' + str(class_idx)\n",
    "            subset = df[(df['layer']==layer) & \n",
    "                    (df['concept_p']==concept_p) &\n",
    "                    (df['concept_n']==concept_n)]\n",
    "            if len(subset) == 0:\n",
    "                print(f'running {layer}, {concept_p}, {concept_n}')\n",
    "                \n",
    "                acts_p,_ = get_acts_for_concept(concept_p,layer)\n",
    "                acts_n,_ = get_acts_for_concept(concept_n,layer)\n",
    "                #_,acts_class = get_acts_for_concept(target_class,layer)\n",
    "                \n",
    "                x = np.concatenate((acts_p,acts_n))\n",
    "                y = np.concatenate((np.zeros(50),np.ones(50)))\n",
    "\n",
    "                x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, stratify=y)\n",
    "\n",
    "                cav_orig,lm= make_orig_cav_and_lm(x_train,y_train)\n",
    "                preds = lm.predict(x_test)\n",
    "                acc_orig = metrics.accuracy_score(y_test,preds)\n",
    "\n",
    "                n_seeds = 50\n",
    "                cav_2_seeds = np.empty((n_seeds,cav_orig.shape[0]))\n",
    "                b = np.empty((n_seeds,1))\n",
    "\n",
    "                for seed in range(n_seeds):\n",
    "                    cav_2_seeds[seed],b[seed] = make_orig_cav_seed(x_train,y_train,seed)\n",
    "\n",
    "                lm1 = linear_model.SGDClassifier(alpha=.1)\n",
    "                lm1.coef_ = np.expand_dims(cav_2_seeds.mean(axis=0),axis=0)\n",
    "                lm1.intercept_ = b.mean(axis=0)\n",
    "                lm1.classes_ = np.array([0., 1.])\n",
    "                preds = lm1.predict(x_test)\n",
    "                acc_marg = metrics.accuracy_score(y_test,preds)\n",
    "\n",
    "                #lm2 = linear_model.SGDClassifier(alpha=.1)\n",
    "\n",
    "                #cav_L1,mu_p,mu_n = make_L1_cav_and_means(x_train,y_train)\n",
    "                #b_ = -.5 * (np.dot(mu_p,mu_p.T) - np.dot(mu_n,mu_n.T))\n",
    "\n",
    "                #lm2.coef_ = -np.expand_dims(cav_L1,axis=0)\n",
    "                #lm2.intercept_ = b_\n",
    "                #lm2.classes_ = np.array([0, 1.])\n",
    "                #preds = lm2.predict(x_test)\n",
    "                #acc_L1 = metrics.accuracy_score(y_test,preds)\n",
    "\n",
    "                this_dict = {'layer':layer,\n",
    "                             'concept_p':concept_p,\n",
    "                             'concept_n':concept_n,\n",
    "                             'acc_orig':acc_orig,\n",
    "                             'acc_marg':acc_marg,}\n",
    "                             #'acc_L1':acc_L1,}\n",
    "                df = df.append([this_dict])\n",
    "            else:\n",
    "                print(f'already in df {layer}, {concept_p}, {concept_n}')\n",
    "            \n",
    "pickle.dump(df,open('df_acc1.pkl', 'wb'))\n",
    "print('### did write ###')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "acts_p,_ = get_acts_for_concept(concept_p,layer)\n",
    "acts_n,_ = get_acts_for_concept(concept_n,layer)\n",
    "_,acts_class = get_acts_for_concept(target_class,layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.concatenate((acts_p,acts_n))\n",
    "y = np.concatenate((np.zeros(50),np.ones(50)))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7272727272727273\n",
      "0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "cav_orig,lm= make_orig_cav_and_lm(x_train,y_train)\n",
    "preds = lm.predict(x_test)\n",
    "acc_orig = metrics.accuracy_score(y_test,preds)\n",
    "print(acc_orig)\n",
    "\n",
    "n_seeds = 50\n",
    "cav_2_seeds = np.empty((n_seeds,cav_orig.shape[0]))\n",
    "b = np.empty((n_seeds,1))\n",
    "\n",
    "for seed in range(n_seeds):\n",
    "    cav_2_seeds[seed],b[seed] = make_orig_cav_seed(x_train,y_train,seed)\n",
    "\n",
    "lm1 = linear_model.SGDClassifier(alpha=.1)\n",
    "lm1.coef_ = np.expand_dims(cav_2_seeds.mean(axis=0),axis=0)\n",
    "lm1.intercept_ = b.mean(axis=0)\n",
    "lm1.classes_ = np.array([0., 1.])\n",
    "preds = lm1.predict(x_test)\n",
    "acc_marg = metrics.accuracy_score(y_test,preds)\n",
    "print(acc_marg)\n",
    "\n",
    "lm2 = linear_model.SGDClassifier(alpha=.1)\n",
    "\n",
    "cav_L1,mu_p,mu_n = make_L1_cav_and_means(x_train,y_train)\n",
    "b_ = -.5 * (np.dot(mu_p,mu_p.T) - np.dot(mu_n,mu_n.T))\n",
    "\n",
    "lm2.coef_ = -np.expand_dims(cav_L1,axis=0)\n",
    "lm2.intercept_ = b_\n",
    "lm2.classes_ = np.array([0, 1.])\n",
    "preds = lm2.predict(x_test)\n",
    "acc_L1 = metrics.accuracy_score(y_test,preds)\n",
    "print(acc_L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8181818181818182"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get activations for concept, layer\n",
    "concept_dir = 'concepts/noise_white'\n",
    "image_path = 'img1.jpg'\n",
    "img = prep2(os.path.join(concept_dir,image_path))\n",
    "this_img = np.expand_dims(img, axis=0)\n",
    "\n",
    "def get_acts_for_concept(concept,layer):\n",
    "    concept_dir = os.path.join(working_dir,'concepts/'+concept)\n",
    "    image_list = files_from_dir_ext(concept_dir,'jp')\n",
    "    image_list.sort()\n",
    "\n",
    "    act_path = os.path.join(working_dir,'final_acts/' + concept + '-' + layer + '.pkl')\n",
    "\n",
    "    n = size_dict[layer]\n",
    "    nn = size_dict_orig[layer]\n",
    "\n",
    "    try:\n",
    "        this_dict = pickle.load(open(act_path, 'rb'))\n",
    "    except:\n",
    "        this_dict = {}\n",
    "        \n",
    "    #print(nn)\n",
    "    \n",
    "    acts_ran = np.zeros((len(image_list),n))\n",
    "    orig = np.zeros((len(image_list),nn[1],nn[2],nn[3]))\n",
    "    \n",
    "    for idx,image_path in enumerate(image_list):\n",
    "        if image_path not in this_dict:\n",
    "            img = prep2(os.path.join(concept_dir,image_path))\n",
    "            this_img = np.expand_dims(img, axis=0)\n",
    "            acts_orig = get_acts_for_layer_new(layer,this_img)\n",
    "            acts_ran[idx] = acts_orig.reshape(-1)\n",
    "            orig[idx] = acts_orig\n",
    "            this_dict[image_path] = (acts_orig.reshape(-1),acts_orig)\n",
    "        else:\n",
    "            acts_ran[idx],orig[idx] = this_dict[image_path]\n",
    "            #print('acts already exist')\n",
    "\n",
    "    pickle.dump(this_dict,open(act_path, 'wb'))\n",
    "    \n",
    "    return acts_ran,orig\n",
    "\n",
    "def make_orig_cav(x_train,y_train):\n",
    "    lm = linear_model.SGDClassifier(alpha=.1)\n",
    "    lm.fit(x_train, y_train)\n",
    "    this_cav = -lm.coef_[0]\n",
    "    return this_cav\n",
    "\n",
    "def make_orig_cav_and_lm(x_train,y_train):\n",
    "    lm = linear_model.SGDClassifier(alpha=.1)\n",
    "    lm.fit(x_train, y_train)\n",
    "    this_cav = -lm.coef_[0]\n",
    "    return this_cav,lm\n",
    "\n",
    "def make_orig_cav_seed(x_train,y_train,seed):\n",
    "    lm = linear_model.SGDClassifier(alpha=.1,random_state=seed)\n",
    "    lm.fit(x_train, y_train)\n",
    "    return lm.coef_,lm.intercept_\n",
    "\n",
    "def make_L1_cav(x_train,y_train):\n",
    "    x_train_p_list,x_train_n_list =[],[]\n",
    "    for idx,a in enumerate(x_train):\n",
    "        if y_train[idx] == 0:\n",
    "            x_train_p_list.append(a)\n",
    "        else:\n",
    "            x_train_n_list.append(a)\n",
    "\n",
    "    x_train_p, x_train_n = np.array(x_train_p_list),np.array(x_train_n_list)\n",
    "    L1_cav = get_L1(x_train_p) - get_L1(x_train_n)\n",
    "    return L1_cav\n",
    "\n",
    "def make_centroid_cav(x_train,y_train):\n",
    "    x_train_p_list,x_train_n_list =[],[]\n",
    "    for idx,a in enumerate(x_train):\n",
    "        if y_train[idx] == 0:\n",
    "            x_train_p_list.append(a)\n",
    "        else:\n",
    "            x_train_n_list.append(a)\n",
    "\n",
    "    x_train_p, x_train_n = np.array(x_train_p_list),np.array(x_train_n_list)\n",
    "    centroid_cav = x_train_p.mean(axis=1) - x_train_n.mean(axis=1)\n",
    "    return centroid_cav\n",
    "\n",
    "def make_L1_cav_and_means(x_train,y_train):\n",
    "    x_train_p_list,x_train_n_list =[],[]\n",
    "    for idx,a in enumerate(x_train):\n",
    "        if y_train[idx] == 0:\n",
    "            x_train_p_list.append(a)\n",
    "        else:\n",
    "            x_train_n_list.append(a)\n",
    "\n",
    "    x_train_p, x_train_n = np.array(x_train_p_list),np.array(x_train_n_list)\n",
    "    mu_p = get_L1(x_train_p)\n",
    "    mu_n = get_L1(x_train_n)\n",
    "    L1_cav = mu_p - mu_n\n",
    "    return L1_cav,mu_p,mu_n\n",
    "\n",
    "def get_L1(act_for_L1):\n",
    "    return np.linalg.norm(act_for_L1.T,1,axis = 1) / act_for_L1.shape[1]\n",
    "\n",
    "def files_from_dir_ext(a_dir,ext):\n",
    "    onlyfiles = [f for f in os.listdir(a_dir) if os.path.isfile(os.path.join(a_dir, f))]\n",
    "    this_ext = [e for e in onlyfiles if ext in e.lower()]\n",
    "    return this_ext\n",
    "\n",
    "acts_mixed0_f = K.function([model.input],[layer_dict['mixed0'].output])\n",
    "acts_mixed1_f = K.function([model.input],[layer_dict['mixed1'].output])\n",
    "acts_mixed2_f = K.function([model.input],[layer_dict['mixed2'].output])\n",
    "acts_mixed3_f = K.function([model.input],[layer_dict['mixed3'].output])\n",
    "acts_mixed4_f = K.function([model.input],[layer_dict['mixed4'].output])\n",
    "acts_mixed5_f = K.function([model.input],[layer_dict['mixed5'].output])\n",
    "acts_mixed6_f = K.function([model.input],[layer_dict['mixed6'].output])\n",
    "acts_mixed7_f = K.function([model.input],[layer_dict['mixed7'].output])\n",
    "acts_mixed8_f = K.function([model.input],[layer_dict['mixed8'].output])\n",
    "acts_mixed9_f = K.function([model.input],[layer_dict['mixed9'].output])\n",
    "acts_mixed10_f = K.function([model.input],[layer_dict['mixed10'].output])\n",
    "\n",
    "    \n",
    "def get_acts_for_layer_new(layer_name,input_img):\n",
    "    acts = None\n",
    "    if layer_name=='mixed0':\n",
    "        acts = acts_mixed0_f([input_img])[0]\n",
    "    if layer_name=='mixed1':\n",
    "        acts = acts_mixed1_f([input_img])[0]\n",
    "    if layer_name=='mixed2':\n",
    "        acts = acts_mixed2_f([input_img])[0]\n",
    "    if layer_name=='mixed3':\n",
    "        acts = acts_mixed3_f([input_img])[0]\n",
    "    if layer_name=='mixed4':\n",
    "        acts = acts_mixed4_f([input_img])[0]\n",
    "    if layer_name=='mixed5':\n",
    "        acts = acts_mixed5_f([input_img])[0]\n",
    "    if layer_name=='mixed6':\n",
    "        acts = acts_mixed6_f([input_img])[0]\n",
    "    if layer_name=='mixed7':\n",
    "        acts = acts_mixed7_f([input_img])[0]\n",
    "    if layer_name=='mixed8':\n",
    "        acts = acts_mixed8_f([input_img])[0]\n",
    "    if layer_name=='mixed9':\n",
    "        acts = acts_mixed9_f([input_img])[0]\n",
    "    if layer_name=='mixed10':\n",
    "        acts = acts_mixed10_f([input_img])[0]\n",
    "    \n",
    "    return acts\n",
    "\n",
    "\n",
    "bn_names = ['mixed0','mixed1','mixed2','mixed3','mixed4','mixed5','mixed6','mixed7','mixed8','mixed9','mixed10']\n",
    "\n",
    "size_dict = {}\n",
    "for bn in bn_names:\n",
    "    acts_orig = get_acts_for_layer_new(bn,this_img)\n",
    "    size_dict[bn] = acts_orig.reshape(-1).shape[0]\n",
    "    \n",
    "size_dict_orig = {}\n",
    "for bn in bn_names:\n",
    "    acts_orig = get_acts_for_layer_new(bn,this_img)\n",
    "    size_dict_orig[bn] = acts_orig.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dict = dict([(layer.name, layer) for layer in model.layers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep2(filename):\n",
    "    shape=(299, 299)\n",
    "    img = np.array(PIL.Image.open(open(filename, 'rb')).convert('RGB').resize(shape, PIL.Image.BILINEAR))\n",
    "    # Normalize pixel values to between 0 and 1.\n",
    "    img = np.float32(img) / 255.0\n",
    "    if not (len(img.shape) == 3 and img.shape[2] == 3):\n",
    "        return None\n",
    "    else:\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
